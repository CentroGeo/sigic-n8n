
services:
  litellm:
    image: ghcr.io/berriai/litellm:main
    container_name: ollama-gemini-proxy
    env_file:
      - .env
    volumes:
      - ./config.yaml:/app/config.yaml:ro
    entrypoint: ["litellm"]
    command:
      - "--config"
      - "/app/config.yaml"
      - "--ollama"
      - "--port"
      - "11434"
    ports:
      - "11434:11434"
    restart: unless-stopped

